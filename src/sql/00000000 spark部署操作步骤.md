

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [spark常用网址](#spark常用网址)
- [spark常用命令](#spark常用命令)
- [hadoop常用命令](#hadoop常用命令)
- [spark部署 - local模式](#spark部署---local模式)
- [spark部署 - yarn模式](#spark部署---yarn模式)
  - [1. 登录nn1，完成环境准备](#1-登录nn1完成环境准备)
  - [2. 提交spark任务 （yarn-client模式）](#2-提交spark任务-yarn-client模式)
  - [3. 提交spark任务（yarn-cluster模式）](#3-提交spark任务yarn-cluster模式)

<!-- /code_chunk_output -->


## spark常用网址

- yarn : http://nn1-87368:8088
- spark: http://nn1-87368:4040
- hdfs : http://nn1-87368:50070
- history: http://nn1-87368:18888
- 

## spark常用命令

```bash

# login in nn1

# 查看各节点jps状况
/root/batch_shell/ssh_all.sh jps

```

## hadoop常用命令

```bash

# hdfs创建文件夹
hadoop fs -mkdir /tecwang

# 递归方式创建多层文件夹
hadoop fs -mkdir -p /tecwang/spark3.1.3

# [root@nn1-25208 ~]# hadoop fs -ls /
# Found 4 items
# drwxrwx---   - root supergroup          0 2024-09-09 16:20 /data
# drwxr-xr-x   - root supergroup          0 2024-09-09 16:26 /tecwang
# drwxr-xr-x   - root supergroup          0 2024-09-09 16:24 /tmp
# drwxr-xr-x   - root supergroup          0 2024-09-09 16:24 /user

# 查看hdfs上的文件

# 本地文件上传到hdfs

# hdfs下载文件到本地




```

## spark部署 - local模式

申请一个最简单的linux环境

并把spark解压缩至对应文件夹，并进行简单配置即可实现

```bash

# 解压缩 spark 至用户目录
tar -zxvf /public/software/bigdata/spark-3.1.3-bin-hadoop2.7.tgz -C /usr/local/

# 创建软链接
ln -s /usr/local/spark-3.1.3-bin-hadoop2.7/ /usr/local/spark-local

# 安装java
# 安装后JAVA_HOME的环境变量会自动配置
rpm -ivh /public/software/language/java/jdk-8u144-linux-x64.rpm

# 添加环境变量
echo export SPARK_HOME=/usr/local/spark-local >> /etc/profile
# 此处需要增加转义符
echo export PATH=\$PATH:\$SPARK_HOME/bin >> /etc/profile

# 环境变量生效
source /etc/profile

# 此时就可以试着提交一些sql任务了
# /usr/local/spark/bin/spark-sql -e "select 1 as col1,2 as col2, 1+9 as col_tmp"

# 也可以试着执行spark提供的样例
# --master表示用master提供资源
    # local表示单线程执行，local[2]表示多线程执行
    # local[*]表示最大线程数
# /usr/local/spark-local/bin/spark-submit \
# --class org.apache.spark.examples.SparkPi \
# --master local[2] \
# /usr/local/spark-local/examples/jars/spark-examples_2.12-3.1.3.jar \
# 1000

# 交互式提交 spark-sql 任务
spark-sql

```

## spark部署 - yarn模式

yarn模式分两类，client模式 和 cluster模式。主要的区别在于 driver运行的位置，如果 driver 运行在 server 端，就是 cluster 模式，如果 driver 运行在 client 端，就是 client模式。

本篇所用的提交模式，就是 client 模式，可以直接在本地看到 spark 任务提交的日志和结果。

需要结合 hadoop2.7.3 环境实施

### 1. 登录nn1，完成环境准备

```bash
# 解压缩 spark 至用户目录
tar -zxvf /public/software/bigdata/spark-3.1.3-bin-hadoop2.7.tgz -C /usr/local/

# 创建软链接
ln -s /usr/local/spark-3.1.3-bin-hadoop2.7/ /usr/local/spark-yarn

# 修改配置信息
cp /usr/local/spark-yarn/conf/spark-env.sh.template /usr/local/spark-yarn/conf/spark-env.sh

# 添加环境变量
echo export SPARK_HOME=/usr/local/spark-yarn >> /etc/profile
# 此处需要增加转义符
echo export PATH=\$PATH:\$SPARK_HOME/bin >> /etc/profile

# 环境变量生效
source /etc/profile

# 追加配置, 告知spark yarn-site的配置在哪里，以便提交任务到yarn
# /usr/local/hadoop-2.7.3/etc/hadoop/yarn-site.xml
echo \# configure yarn-site.xml path for submit jobs to yarn >> /usr/local/spark-yarn/conf/spark-env.sh
echo YARN_CONF_DIR=/usr/local/hadoop-2.7.3/etc/hadoop >> /usr/local/spark-yarn/conf/spark-env.sh

# 启动代理服务，否则无法跳转 Tracking Url
# /usr/local/hadoop-2.7.3/sbin/yarn-daemon.sh start proxyserver

####################### optional configure start #######################

# 调整spark配置文件，支持spark web ui历史记录访问呢
cp /usr/local/spark-yarn/conf/spark-defaults.conf.template /usr/local/spark-yarn/conf/spark-defaults.conf

# 创建日志文件夹
hadoop fs -mkdir /directory

# 追加配置
# hdfs的路径应该是 hdfs://ns1/ 而非 nn1 ，否则无法连接。Error info: Log directory specified does not exist: file:/tmp/spark-events
# ns1的地址可以在 hadoop 的 core-site.xml 文件中得到，defaultFS
echo spark.eventLog.enabled true >> /usr/local/spark-yarn/conf/spark-defaults.conf
echo spark.eventLog.dir hdfs://ns1/directory >> /usr/local/spark-yarn/conf/spark-defaults.conf
# 注意此处不是 ss1 的地址，要的是服务器的地址
echo spark.yarn.historyServer.address=${HOSTNAME}:18888 >> /usr/local/spark-yarn/conf/spark-defaults.conf
echo spark.history.ui.port=18888 >> /usr/local/spark-yarn/conf/spark-defaults.conf

# 历史服务器已经可以正常运行，并且日志也可以写入hdfs目录，但是访问 history web 页面还是无法正常展示已经完成的 job，显示 job not found，留待有精力时再研究。
  # 已解决，因为下方的 SPARK_HISTORY_OPTS 拼写错误，写成了SPARK_HISROTY_OPTS

# 追加配置
echo \# enable spark history log >> /usr/local/spark-yarn/conf/spark-env.sh
echo export SPARK_HISTORY_OPTS=\" >> /usr/local/spark-yarn/conf/spark-env.sh
echo -Dspark.history.ui.port=18888 >> /usr/local/spark-yarn/conf/spark-env.sh
echo -Dspark.history.fs.logDirectory=hdfs://ns1/directory >> /usr/local/spark-yarn/conf/spark-env.sh
echo -Dspark.history.retainedApplications=30\" >> /usr/local/spark-yarn/conf/spark-env.sh

# 创建 spark-events 文件夹，否则会报错。Error info: Log directory specified does not exist: file:/tmp/spark-events
mkdir /tmp/spark-events

# 重启 hadoop服务（需要在nn1执行）
/usr/local/hadoop-2.7.3/sbin/stop-dfs.sh
/usr/local/hadoop-2.7.3/sbin/stop-yarn.sh
/usr/local/hadoop-2.7.3/sbin/start-dfs.sh
/usr/local/hadoop-2.7.3/sbin/start-yarn.sh

# 历史服务需要在主服务重启完成后启动
# 重启 hadoop历史服务
/usr/local/spark-yarn/sbin/stop-history-server.sh
/usr/local/spark-yarn/sbin/start-history-server.sh

####################### optional configure end #######################

```

### 2. 提交spark任务 （yarn-client模式）

```bash

# 提交spark任务
# 1 增加了一些 executors的参数，用于适配海牛的环境，资源太少了。
# 2 指定了任务的提交队列，否则任务会被拒绝
# 3 为什么要设置 executor 的内存为 620MB ?
    # 海牛设置的 yarn 的资源上线为 1024MB, 并且预留了 overhead memory 384MB， 则 1024 - 384 = 640 MB
/usr/local/spark-yarn/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--num-executors 3 \
--executor-cores 1 \
--driver-memory 620m \
--executor-memory 620MB \
--queue hainiu \
/usr/local/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar \
100
# 以下为注释掉的参数，如有需要可以移上去
# --verbose \

运行一个任务至少需要两个 execturor，一个作为 driver，一个作为 executor。

# ssh root@nn1-42691 "jps"
# 3537 DFSZKFailoverController
# 3687 ResourceManager
# 3849 WebAppProxyServer
# 3275 JournalNode
# 11051 SparkSubmit
# 140 QuorumPeerMain
# 11615 Jps
# OK
# ssh root@nn2-42691 "jps"
# 129 QuorumPeerMain
# 945 JournalNode
# 2114 Jps
# 805 NameNode
# 1079 DFSZKFailoverController
# OK
# ssh root@s1-42691 "jps"
# 785 DataNode
# 3889 Jps
# 131 QuorumPeerMain
# 3708 YarnCoarseGrainedExecutorBackend
# 941 NodeManager
# OK
# ssh root@s2-42691 "jps"
# 3408 ExecutorLauncher
# 839 NodeManager
# 3735 Jps
# 683 DataNode
# OK
# ssh root@s3-42691 "jps"
# 836 NodeManager
# 3350 Jps
# 680 DataNode
# 3167 YarnCoarseGrainedExecutorBackend

可以看到 s2 运行了 ExecutorLauncher 进程，s1，s3 运行了 YarnCoarseGrainedExecutorBackend 进程。

# ERROR 1: Rejected by queue placement policy
# Error info: 24/09/08 11:16:44 INFO spark.SparkContext: Successfully stopped SparkContext. Exception in thread "main" org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1725765237665_0001 to YARN : Application rejected by queue placement policy
# Solution: 在提交spark任务时，指定目标队列为hainiu即可。

# Error 2: 提交spark任务后，history web ui无法访问
# State  : 尚未解决，原因还在排查中，以下记录排查过程

step 1: web ui 无法访问，首先还是查看日志。history log 日志没有看到任何异常。

cat /usr/local/spark-yarn/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-nn1-89249.out

# 24/09/11 10:05:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b324585{/,null,AVAILABLE,@Spark}
# 24/09/11 10:05:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@192d74fb{/json,null,AVAILABLE,@Spark}
# 24/09/11 10:05:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62ea3440{/api,null,AVAILABLE,@Spark}
# 24/09/11 10:05:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@666b83a4{/static,null,AVAILABLE,@Spark}
# 24/09/11 10:05:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1dd0e7c4{/history,null,AVAILABLE,@Spark}
# 24/09/11 10:05:12 INFO history.HistoryServer: Bound HistoryServer to 0.0.0.0, and started at http://nn1-89249:18888

step 2: 怀疑是内存不足导致 history 进程崩溃。以下日志是正常情况下的日志，web ui无法访问时，无法看到7844进程。

ps -ef | grep history

# [root@nn1-89249 logs]# ps -ef | grep history
# root      7844     1  1 10:05 pts/0    00:00:08 /opt/jdk1.8.0_144/bin/java -cp /usr/local/spark-yarn/conf/:/usr/local/spark-yarn/jars/*:/opt/hadoop-2.7.3/etc/hadoop/:/usr/local/hadoop-2.7.3/etc/hadoop/ -Xmx1g org.apache.spark.deploy.history.HistoryServer
# root      8486  6339  0 10:15 pts/0    00:00:00 grep --color=auto history

step 3: 查看端口是否正在监听。配置文件中设置的 web ui 端口为 18888，以下是正常情况下的日志，web ui 无法访问时，无法看到 18888 端口正在监听。

netstat -nl -p

# Active Internet connections (only servers)
# Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
# tcp        0      0 0.0.0.0:8485            0.0.0.0:*               LISTEN      3495/java           
# tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      140/java            
# tcp        0      0 0.0.0.0:18888           0.0.0.0:*               LISTEN      7844/java           
# tcp        0      0 11.67.196.55:8041       0.0.0.0:*               LISTEN      4068/java           
# tcp        0      0 11.67.196.55:3888       0.0.0.0:*               LISTEN      140/java            
# tcp        0      0 11.67.196.55:8019       0.0.0.0:*               LISTEN      3757/java           
# tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      17/sshd             
# tcp        0      0 11.67.196.55:8088       0.0.0.0:*               LISTEN      3907/java           
# tcp        0      0 0.0.0.0:41181           0.0.0.0:*               LISTEN      140/java            
# tcp        0      0 11.67.196.55:8030       0.0.0.0:*               LISTEN      3907/java           
# tcp        0      0 11.67.196.55:8031       0.0.0.0:*               LISTEN      3907/java           
# tcp        0      0 11.67.196.55:8032       0.0.0.0:*               LISTEN      3907/java           
# tcp        0      0 0.0.0.0:8480            0.0.0.0:*               LISTEN      3495/java           
# tcp        0      0 11.67.196.55:8033       0.0.0.0:*               LISTEN      3907/java           
# tcp6       0      0 :::22                   :::*                    LISTEN      17/sshd  

step 4: 查看进程运行过程中，7844进程的内存占用情况。首先需要根据 $JAVA_HOME 拿到 java的所在目录，然后执行以下命令。

可以根据以下命令查看当前java进程的相关参数

jps -v | grep 7844

# 7844 HistoryServer -Dspark.history.ui.port=18888 -Dspark.history.fs.logDirectory=hdfs://ns1/directory -Dspark.history.retainedApplications=30 -Xmx1g

/opt/jdk1.8.0_144/bin/jstat -gcutil 7844 1000

S0：幸存1区当前使用比例
S1：幸存2区当前使用比例
E：伊甸园区使用比例
O：老年代使用比例
M：元数据区使用比例
CCS：压缩使用比例
YGC：年轻代垃圾回收次数
FGC：老年代垃圾回收次数
FGCT：老年代垃圾回收消耗时间
GCT：垃圾回收消耗总时间

从结果来看，伊甸园、老年代的比例都不高，应该不是内存不足的原因。

  # S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   
  # 0.00   0.00  35.41   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  35.41   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  43.47   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  49.27   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  49.67   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  50.07   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  50.07   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  50.47   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  50.47   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  50.87   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  51.27   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  52.07   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  52.47   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  52.47   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  52.87   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  52.87   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  53.27   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  53.27   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  54.07   3.61  96.02  95.45      1    0.505     1    0.665    1.170
  # 0.00   0.00  54.07   3.61  96.02  95.45      1    0.505     1    0.665    1.170

```

### 3. 提交spark任务（yarn-cluster模式）

```bash

/usr/local/spark-yarn/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
--num-executors 3 \
--executor-cores 1 \
--driver-memory 512m \
--executor-memory 620MB \
--queue hainiu \
--verbose \
/usr/local/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar \
1000

# ssh root@nn1-42691 "jps"
# 14213 DFSZKFailoverController
# 14600 SparkSubmit
# 14985 Jps
# 13562 NameNode
# 140 QuorumPeerMain
# 14364 ResourceManager
# 13950 JournalNode
# OK
# ssh root@nn2-42691 "jps"
# 129 QuorumPeerMain
# 2787 Jps
# 2293 NameNode
# 2421 JournalNode
# 2556 DFSZKFailoverController
# OK
# ssh root@s1-42691 "jps"
# 131 QuorumPeerMain
# 4743 Jps
# 4235 NodeManager
# 4078 DataNode
# 4638 YarnCoarseGrainedExecutorBackend
# OK
# ssh root@s2-42691 "jps"
# 4579 Jps
# 4073 NodeManager
# 3916 DataNode
# 4476 YarnCoarseGrainedExecutorBackend
# OK
# ssh root@s3-42691 "jps"
# 3690 NodeManager
# 4090 ApplicationMaster
# 4316 Jps
# 3533 DataNode
# OK

提交模式改成 cluster 之后， client 被提交到了 s3 节点，进程名叫 ApplicationMaster。其他的没有变化，executor 的进程还是 YarnCoarseGrainedExecutorBackend。

# cluster 模式
INFO yarn.Client: 
         client token: N/A
         diagnostics: N/A
         ApplicationMaster host: s3-42691
         ApplicationMaster RPC port: 40797
         queue: root.hainiu
         start time: 1726040148084
         final status: SUCCEEDED
         tracking URL: http://nn1-42691:8041/proxy/application_1726040121270_0001/
         user: root

# client 模式
INFO yarn.Client: 
         client token: N/A
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.hainiu
         start time: 1726040783179
         final status: UNDEFINED
         tracking URL: http://nn1-42691:8041/proxy/application_1726040121270_0003/
         user: root

可以看到两个模式的区别主要在于 applicationMaster 的信息，cluster 模式的服务器在 s3 节点。

```

如果设置了 ```num-executors=2, executor-cores=2```，则yarn上显示一共占用了 5 vc的资源。内存占用量为3072 MB。

$$\begin{align}
5 vc & = driver(1) + NumExecutors(2) * ExecutorCores(2) \\
     & = 1 vc + 4 vc \\
     & = 1 * 592 + 4 * 620 = 3072
\end{align}$$

<!-- ? 为什么 driver 的内存数量没有对上 -->

## spark打包并在集群环境执行

### 1. 在 idea 中打开 maven，执行 package 命令。

打包好的 jar 在 target目录中，检查目标执行类是包含在 jar 包中（通过压缩软件就可以查看）。

### 2. 启动 spark 环境，上传 jar 到  文件夹

目前没有找到可以直接上传文件的路径，需要

1. 先把文件上传到 gituhub。

2. 下载到远程桌面所在的节点，并解压缩找到对应的 jar 包

3. 在文件所在文件夹右键打开命令行，利用`scp`命令传输。`scp learn-1.0-SNAPSHOT.jar root@nn1-97654:/usr/local/`

4. 登录nn1节点，将jar包放在目标路径：`/usr/local/spark-yarn/examples/jars/`

```bash

# 提交任务
# 确保类名要存在
/usr/local/spark-yarn/bin/spark-submit \
--class base.spark.BSparkRDDOperateTransformGroupByKey --master yarn \
--num-executors 3 \
--executor-cores 1 \
--driver-memory 620m \
--executor-memory 620MB \
--queue hainiu \
/usr/local/spark-yarn/examples/jars/learn-1.0-SNAPSHOT.jar
```


