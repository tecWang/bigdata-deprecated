
### 1. 安装docker

### 2. 创建容器

```bash

# 拉取镜像
docker pull harisekhon/hadoop

# 运行容器 
# 8081是flink需要的端口
docker run -ti  --name hadoop  -p 8042:8042  -p 8088:8088  -p 19888:19888  -p 50070:50070  -p 50075:50075  -p 8081:8081 harisekhon/hadoop

# 退出容器后创建一个bash窗口继续安装hive
# 退出的命令是 ctrl + p + ctrl + q
docker exec -it hadoop /bin/bash

```

### 3. 安装hive

```bash

# wget command not found
# 需要先安装更新镜像源，并安装wget

# 备份原有镜像
cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak

# 下载新镜像
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

# 确认镜像内容是否更新
cat /etc/yum.repos.d/CentOS-Base.repo

# 清空缓存
yum clean all
yum makecache

# 安装wget
yum install wget

# 下载文件并指定下载路径
wget -P /usr/local https://mirrors.aliyun.com/apache/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# 解压缩
tar -zxvf apache-hive-3.1.3-bin.tar.gz -C /usr/local/

# 重命名
mv apache-hive-3.1.3-bin hive-3.1.3

# 添加环境变量
echo export HIVE_HOME=/usr/local/hive-3.1.3 >> /etc/profile
# 此处需要增加转义符
echo export PATH=\$PATH:\$HIVE_HOME/bin >> /etc/profile

# 环境变量生效
source /etc/profile

# hive-site.xml
# /usr/local/hive-3.1.3/conf/hive-site.xml

echo \<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<configuration\> >> /usr/local/hive-3.1.3/conf/hive-site.xml

echo \<property\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<name\>hive.cli.print.header\</name\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<value\>true\</value\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<description\>Whether to print the names of the columns in query output.\</description\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \</property\> >> /usr/local/hive-3.1.3/conf/hive-site.xml

echo \<property\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<name\>hive.cli.print.current.db\</name\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<value\>true\</value\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<description\>Whether to include the current database in the Hive prompt.\</description\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \</property\> >> /usr/local/hive-3.1.3/conf/hive-site.xml

echo \<property\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<name\>hive.resultset.use.unique.column.names\</name\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<value\>false\</value\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \<description\>Whether to include the current database in the Hive prompt.\</description\> >> /usr/local/hive-3.1.3/conf/hive-site.xml
echo \</property\> >> /usr/local/hive-3.1.3/conf/hive-site.xml

echo \</configuration\> >> /usr/local/hive-3.1.3/conf/hive-site.xml

# 直接初始化元数据库会报错，删除低版本guava文件
rm -f /usr/local/hive-3.1.3/lib/guava-19.0.jar
# 并把高版本文件复制到hive文件夹
cp /hadoop-2.8.2/share/hadoop/common/lib/guava-11.0.2.jar /usr/local/hive-3.1.3/lib/

# 初始化元数据库
# dbType 选择了 hadoop 自带的 derby 数据库来存储元数据，也可以选择 mysql，mysql需要额外安装。
/usr/local/hive-3.1.3/bin/schematool -dbType derby -initSchema

# 直接在命令行输入hive即可
hive

```

### 4. 关闭容器并重新启动

```bash

# 停止容器
docker stop hadoop

# 重新启动容器
docker start -ia hadoop

# 新开一个命令行
C:\Users\rumor>docker exec -it hadoop /bin/bash

# 需要重新激活环境变量

# [root@7e99abbbf695 /]# cd /usr/local/
# [root@7e99abbbf695 local]# ll
# total 319352
# -rw-r--r--  1 root root 326940667 Apr  8  2022 apache-hive-3.1.3-bin.tar.gz
# drwxr-xr-x  2 root root      4096 Nov  5  2016 bin
# -rw-r--r--  1 root root     22193 Oct  7 12:38 derby.log
# drwxr-xr-x  2 root root      4096 Nov  5  2016 etc
```

### 5. 安装 flink

```bash

wget -P /usr/local/ https://mirrors.aliyun.com/apache/flink/flink-1.20.0/flink-1.20.0-bin-scala_2.12.tgz

cd /usr/local/flink-1.20.0/bin

# flink 默认监听 127.0.0.1，如果想要在主机访问容器的8081服务的
# 需要修改 config.yaml 
# rest:
  # The address that the REST & web server binds to
  # By default, this is localhost, which prevents the REST & web server from
  # being able to communicate outside of the machine/container it is running on.
  #
  # To enable this, set the bind address to one that has access to outside-facing
  # network interface, such as 0.0.0.0.
#   bind-address: 0.0.0.0

./start-cluster.sh

# [root@7e99abbbf695 bin]# jps
# 2421 StandaloneSessionClusterEntrypoint   -- flink
# 902 NodeManager
# 281 DataNode
# 2970 TaskManagerRunner                    -- flink
# 490 SecondaryNameNode
# 3082 Jps
# 763 ResourceManager
# 141 NameNode
# 1295 RunJar

# 由于flink需要使用8081端口，因此需要在容器上新增8081端口的映射

# 停止并移除现有容器
docker stop hadoop
docker remove hadoop

# 提交为新镜像
docker commit hadoop hadoop-flink

# 重新提交任务
docker run -ti --name hadoop -p 8042:8042 -p 8088:8088 -p 19888:19888 -p 50070:50070 -p 50075:50075 -p 8081:8081 hadoop-flink

```